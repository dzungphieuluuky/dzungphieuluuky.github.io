---
layout: post
title: BF16 vs. FP16 vs. FP32
subtitle: Look into the data types of AI training and inference
tags: [deep-learning, machine-learning, floating-point, bf16, fp16, fp32, mixed-precision, hardware-acceleration, pytorch]
comments: true
author: dzungphieuluuky
---

{: .box-success}
**Quick Note:**  
This post breaks down the key differences between FP32, FP16, and BF16 floating-point formats, with a focus on why BF16 has become the go-to choice for large-scale deep learning. We will delve into the difference between three most popular data types for deep learning projects: BF16, FP16 and FP32 and why we should switch to BF16 for our next projects.

**BFloat16 (BF16) strikes the ideal balance for deep learning: it preserves the vast dynamic range of FP32 while halving memory and compute requirements, avoiding the overflow issues that plague FP16 during training.**

## Floating-Point Basics: Why Precision Matters in Deep Learning

Deep learning models process billions of parameters (now it can be over 1 trillion parameters) and perform trillions of operations. The choice of numeric format directly affects memory usage, computational speed, training stability, and final accuracy.

Traditional single-precision (FP32) has long been the default, offering robust numerical behavior. As models grew larger (think GPT-4-scale and beyond), researchers turned to lower-precision formats to reduce memory footprint, leverage hardware accelerators and spend less time on both training and inference for more cost-efficient computing. Two 16-bit contenders emerged: IEEE FP16 and Google's Brain Floating Point 16 (BF16).

![FP32, BF16, and FP16 bit layouts compared](https://www.researchgate.net/publication/385701254/figure/fig1/AS:11431281289576710@1731294928530/Representation-of-FP32-BF16-FP16-floating-point-types.png){: .mx-auto.d-block :}

### Bit Allocation Comparison

| Format | Total Bits | Sign | Exponent | Mantissa | Approx. Dynamic Range       | Decimal Digits of Precision |
|--------|------------|------|----------|----------|-----------------------------|-----------------------------|
| FP32   | 32         | 1    | 8        | 23       | ±3.4 × 10³⁸                 | ~7                          |
| FP16   | 16         | 1    | 5        | 10       | ±6.1 × 10⁴                  | ~3–4                        |
| BF16   | 16         | 1    | 8        | 7        | ±3.4 × 10³⁸ (same as FP32) | ~2–3                        |

{: .box-note}
**Note:** BF16 uses the same exponent bits as FP32, giving it identical dynamic range but coarser precision due to the reduced mantissa.

## The Critical Trade-off: Dynamic Range vs. Precision

Deep learning training involves values spanning many orders of magnitude—tiny gradients during early layers, large activations in later ones. FP16's narrow exponent range often causes overflow or underflow, leading to NaN losses or stalled training, greatly affects the stability during training and prevents convergence.

BF16 sacrifices some mantissa precision (which deep networks are remarkably tolerant of) to keep FP32's full exponent range. This makes gradients and loss scaling far more stable. With this improvements, we no longer to rely on external loss scaling techniques or other methods to bring back the training stability, thus reduce the technical efforts and allow our attention solely focus on core theory and logic.

![Dynamic range visualization (FP16 limited, BF16 matches FP32)](https://images.contentstack.io/v3/assets/blt71da4c740e00faaa/blt40c8ab571893763a/65f370cc0c744dfa367c0793/EXX-blog-fp64-fp32-fp-16-5_(3).jpg?format=webp){: .mx-auto.d-block :}

```python
import torch

# A large number that fits in FP32 and BF16 but overflows in FP16
large = torch.tensor(1e30, dtype=torch.float32)
print("FP32:", large)
print("BF16:", large.bfloat16())
print("FP16:", large.half())  # Returns inf
```

Output:
```
FP32: tensor(1e+30)
BF16: tensor(1e+30, dtype=torch.bfloat16)
FP16: tensor(inf, dtype=torch.float16)
```

{: .box-warning}
**Warning:** Pure FP16 training frequently requires careful loss scaling and gradient clipping to avoid NaN issues—extra complexity that BF16 largely eliminates. Less code, less bugs.

## Hardware Acceleration and Real-World Performance

Modern accelerators are heavily optimized for lower precision:

- NVIDIA Ampere (A100) and later → native BF16 tensor cores with near-FP16 throughput
- AMD Instinct MI250/MI300 → full BF16 support
- Google Cloud TPUs (v3 onward) → originally designed around BF16. Google Brain is the one gives birth to Bf16 so, of course.
- Intel Xeon with AMX extensions → BF16 matrix operations

![Performance gains from mixed BF16 training on TPUs](https://storage.googleapis.com/gweb-cloudblog-publish/images/Performance_improvements_from_mixed_precis.max-1200x1200.png){: .mx-auto.d-block :}

```python
import torch
from torch.cuda.amp import autocast

model = MyModel().cuda()
optimizer = torch.optim.AdamW(model.parameters())

# Modern mixed-precision training with BF16 (no GradScaler needed)
with autocast(device_type='cuda', dtype=torch.bfloat16):
    outputs = model(inputs)
    loss = criterion(outputs, targets)

loss.backward()
optimizer.step()
```

```python
# Older FP16 approach required explicit GradScaler
from torch.cuda.amp import GradScaler

scaler = GradScaler()

with autocast(device_type='cuda', dtype=torch.float16):
    outputs = model(inputs)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

{: .box-success}
On supported hardware, switching to BF16 often gives 2–3× speedup and half the memory usage with negligible accuracy drop.

Deep networks are surprisingly robust to reduced mantissa precision because weight updates rely more on direction than magnitude. Empirical studies (e.g., training ResNet-50, BERT-large, and GPT-class models) show BF16 reaches FP32 accuracy with far fewer tuning headaches.

## Conclusion: BF16 as the industry standard

BF16 has largely replaced pure FP16 for training large models. Therefore, if we play around some popular deep learning libraries and frameworks such as **unsloth** or **HuggingFace** libraries, we often see an option called **mixed-precision** whihch allows us to specify which one to use (fp16, bf16, fp32) for more time and cost efficient finetuning. It delivers:

- FP32-like numerical stability due to possessing a wide range of numerical representation.  
- Near-FP16 speed and memory efficiency due to having the same number of bits as FP16.  
- Simpler code, no loss scaling or extra processing code required, less code therefore less bugs.  
- Broad hardware support across NVIDIA, AMD, Google, and Intel for more efficient finetuning for very large models at industry level.

FP16 remains useful for inference on edge devices, while FP32 is still the gold standard when maximum accuracy is paramount and when our devices used for training do not support natively for BF16.

**For most deep learning practitioners today, BF16 is the balance point between performance, stability, and ease of use.**